\cardfrontfoot{Lineare Gleichungssysteme}

\begin{flashcard}[Algorithmus]{Rückwärtssubstitution}
\begin{algorithm}[H]
	\KwIn{$R, y$}
	\KwOut{$x$}
	$x_n = y_n/r_{nn};$\\
	\For{$i = n-1, \ldots, 1$}{
		$sum = r_{in}x_n;$\\
		\For{$j = i+1; \ldots, n-1$}{
			$sum = sum + r_{ij}x_j;$
			}
		$x_i = (y_i - sum) / r_{ii};$
		}
\end{algorithm}
\end{flashcard}

\begin{flashcard}[Algorithmus]{LR-Zerlegung}
\begin{enumerate}
	\item Bestimme Matrizen $P$, $L$ und $R$ mit $PA = LR$ durch Gauß-Elimination.
	\item Löse $Ly = Pb$ (Vorwärtssubstitution).
	\item Löse $Rx = y$ (Rückwärtssubstitution).
\end{enumerate}
\end{flashcard}

\begin{flashcard}[Aufwand]{Vorwärts-, Rückwärtssubstitution}
$$\sum_{i=1}^{n-1}i = n \frac{n-1}{2} \approx \frac{n^2}{2}$$
\end{flashcard}

\begin{flashcard}[Aufwand]{LR-Zerlegung}
$$\sum_{j=1}^{n-1}j^2 = \frac{(n-1)n(2n-1)}{6} \approx \frac{n^3}{3}$$
\end{flashcard}

\begin{flashcard}{Spaltenpivotwahl LR-Zerlegung}
$$|a_{jk}^{(k-1)}| = \max_{k \leq i \leq n} |a_{ik}^{(k-1)}|$$
Dies führt zu:\\
$l_{ij} \leq 1$ für alle $i, j$
\end{flashcard}

\begin{flashcard}[Definition]{zugehörige Matrixnorm}
Sei ||.|| eine beliebige Norm auf $\mathbb{R}^n$. Dann definieren wir die \textbf{zugehörige Matrixnorm} auf dem Raum der quadratischen $(n \times n)$-Matrizen durch
$$||A|| := \sup_{x \neq 0} \frac{||Ax||}{||x||} \text{für} A \in \mathbb{R}^{n \times n} \text{.}$$
\end{flashcard}

\begin{flashcard}{Submultiplikativität der Matrixnorm}
$$||I|| = 1,$$\\
$$||A \cdot B|| \leq ||A|| \cdot ||B||.$$
\end{flashcard}

\begin{flashcard}[Satz]{Matrixnormen}
Sei $A$ eine quadratische $(n \times n)$-Matrix. Es gilt:
\begin{enumerate}[(i)]
	\item $\displaystyle ||A||_1 = \max_{j=1,\ldots,n} \sum_{i=1}^n |a_{ij}| \ \text{(Spaltensummennorm)}$
	\item $\displaystyle ||A||_2 = \sqrt{\text{größter EW von $A^T$A}} \ \text{(Spektralnorm)}$
	\item $\displaystyle ||A||_\infty = \max_{i=1,\ldots,n} \sum_{j=1}^n |a_{ij}| \ \text{(Zeilensummennorm)}$
\end{enumerate}
\end{flashcard}

\begin{flashcard}[Satz]{über die Cholesky-Zerlegung}
Eine invertierbare Matrix $A$ besitzt genau dann eine Cholesky-Zerlegung $A = LL^T$ mit unterer Dreiecksmatrix $L$, wenn $A$ eine spd-Matrix ist.
\end{flashcard}

\begin{flashcard}[Algorithmus]{Cholesky-Verfahren für spd-Matrizen}
Zum Lösen von
$$Ax = L \underbrace{L^T x}_{=:y} = b$$
gehen wir wie folgt vor:
\begin{enumerate}
	\item Löse durch Vorwärtssubstitution $Ly = b$.
	\item Löse durch Rückwärtssubstitution $L^Tx = y$.
\end{enumerate}
\end{flashcard}

\begin{flashcard}[Algorithmus]{Cholesky-Zerlegung}
\begin{algorithm}[H]
	\KwIn{$A$}
	\KwOut{$L$}
	Berechne $L(1,1) = \sqrt{A(1,1)}$;\\
	\For{$k = 2, \ldots, n$}{
		Löse $L(1:k-1,1:k-1)x = A(1:k-1,k)$ durch Vorwärtssubstitution;\\
		Setze $L(k,1:k-1) = x^T$;\\
		Berechne $L(k,k) = \sqrt{A(k,k) - x^Tx}$;
	}
\end{algorithm}
\end{flashcard}

\begin{flashcard}[Satz]{Gauß (Lineare Ausgleichsprobleme)}
Seien $A \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m$ mit $m > n$. Der Vektor $x \in \mathbb{R}^n$ ist genau dann eine Lösung des linearen Ausgleichsproblems $||Ax -b||_2 = min$, falls er die sogenannte Normalengleichung
$$A^TAx = A^Tb$$
löst. Insbesondere ist das lineare Ausgleichsproblem genau dann eindeutig lösbar, wenn der Rang $A$ maximal ist, d.h. $Rang(A) = n$ gilt.
\end{flashcard}

\begin{flashcard}[Definition]{Kondition einer Matrix}
	Wir nennen
	$$ cond(A) := \| A \| \|A^{-1}\| $$
	die \emph{Konditionszahl} der Matrix $A$.
	Sie ist ein Maß der Sensitivität des relativen Fehlers gegenüber relativen Störungen der rechten Seite $b$ [eines Gleichungssystems].
\end{flashcard}

\begin{flashcard}[Satz]{Störungen von $A$ und $b$}
	Sei $A$ eine invertierbare Matrix und
	$$ Ax = b, \qquad \bar{A}\bar{x}=\bar{b}.$$
	Seien weiter die relativen Abweichungen beschränkt:
	$$ \frac{\|A - \bar{A}\|}{\|A\|} \leq \epsilon_A, \qquad \frac{\|b - \bar{b}\|}{\|b\|} \leq \epsilon_b.$$
	Dann gilt die Abschätzung:
	$$ \frac{\|x - \bar{x}\|}{\|x\|} \leq \frac{cond(A)}{1 - \epsilon_A \cdot cond(A)} \cdot (\epsilon_A + \epsilon_b),$$
	falls $\epsilon_A \cdot cond(A) < 1$.
\end{flashcard}

\begin{flashcard}[Definition]{Stabilität (Vorwärtsanalyse)}
	Der Algorithmus heißt \emph{numerisch stabil Vorwärtsanalyse}, falls
	$$ \frac{\|x - \hat{x}\|}{\|x\|} \leq C \cdot cond(A) \cdot eps$$
	mit nicht allzu großem C gilt.
	D.h. der Einfluss von Rundungsfehlern während der Rechnung ist nicht viel größer als der Einfluss von Rundungsfehlern in den Daten.
\end{flashcard}

\begin{flashcard}[Definition]{Stabilität (Rückwärtsanalyse)}
	Der Algorithmus heißt \emph{numerisch stabil Vorwärtsanalyse}, falls das numerische Ergebnis $\hat{x}$ als exakte Lösung einer Gleichung $\bar{A}\hat{x} = \bar{b}$ interpretiert werden kann mit
	$$ \frac{\|A - \bar{A}\|}{\|A\|} \leq C \cdot eps, \qquad \frac{\|b - \bar{b}\|}{\|b\|} \leq C \cdot eps$$
	mit nicht allzu großem C.

	Die Idee besteht darin, die durch den Algorithmus verursachten Fehler auf die Eingabegröße zurückzuspielen und so als zusätzliche Eingabefehler zu interpretieren.
\end{flashcard}

\begin{flashcard}[Satz]{Rückwärtsanalyse der Gauß-Elimination ohne Pivotwahl}
	Seien $A \in \mathbb{R}^{n \times n}$ eine Matrix und $b \in \mathbb{R}^n$ ein Vektor von Gleitpunktzahlen.
	Des Weiteren besitze $A$ eine LR-Zerlegung und es seien $\hat{L}, \hat{R}$ in Gleitpunkt-Arithmetik berechnet.
	Das in Gleitpunkt-Arithmetik erhaltene Ergebnis $\hat{x}$ von $\hat{L}\hat{c} = b, \hat{R}x = \hat{c}$ erfüllt
	$$ \bar{A}\hat{x} = b $$
	für eine Matrix $\bar{A}$ mit
	$$ |A - \bar{A}| \leq 3 (n + 1) eps |\hat{L}| |\hat{R}| + \mathcal{O}(eps^2).$$
\end{flashcard}

\begin{flashcard}[Satz]{Rückwärtsanalyse der Gauß-Elimination mit Spaltenpivotwahl}
	Seien $A \in \mathbb{R}^{n \times n}$ eine Matrix und $b \in \mathbb{R}^n$ ein Vektor von Gleitpunktzahlen.
	Des Weiteren sei die Gauß-Elimination mit Spaltenpivotwahl durchführbar.
	Für das Gleichungssystem $Ax = b$ berechnet sie in Gleitpunkt-Arithmetik ein $\hat{x}$, so dass
	$$ \bar{A}\hat{x} = b $$
	für eine Matrix $\bar{A}$ mit
	$$ \frac{\|A - \bar{A}\|_\infty}{\|A\|_\infty} \leq 2 n^3 \frac{\max_{i,j} |\hat{r}_{ij}|}{\max_{i,j} |a_{ij}|} eps + \mathcal{O}(eps^2).$$
\end{flashcard}

\begin{flashcard}[Definition]{QR-Zerlegung}
	Zu einer gegebenen Matrix $A \in \mathbb{R}^{m \times n}$ mit $m \geq n$ lässt sich eine Zerlegung
	$$ A = QR $$
	konstruieren mit orthogonaler Matrix $Q \in \mathbb{R}^{m \times m}$ (d.h. $QQ^T = I$ und
$$ R = \begin{pmatrix}\tilde{R}\\0\end{pmatrix} \in \mathbb{R}^{m \times n}, \tilde{R} \in \mathbb{R}^{n \times n} \text{ obere Dreiecksmatrix.}$$
\end{flashcard}

\begin{flashcard}[Algorithmus]{QR-Zerlegung}
	\begin{enumerate}
		\item Bestimme $Q$ und $R$ mittels Householder-Transformation mit $A = QR$.
		\item Löse $Qc = b$ mit $c = Q^{-1} b = Q^T b$.
		\item Löse $\tilde{R}x = c$ (Rückwärtssubstitution). Für $m = n$ gilt $R = \tilde{R}$.
	\end{enumerate}
\end{flashcard}
